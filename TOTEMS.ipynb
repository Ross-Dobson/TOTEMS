{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TOTEMS - Tidal Orbital decay Timing Extrapolation & Modelling Software\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit #Import curve fitting (Which is what we need for tidal decay)\n",
    "\n",
    "import pylightcurve as plc # import pylightcurve - used for BJD HJD conversions - Angelos Tsiaras"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%matplotlib notebook\n",
    "# if this isn't in a seperate cell, sometimes doesn't work right\n",
    "# magic commands are weird, to say the least.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "First function: gets the relevant data for the exoplanet from the Exoplanet Transit Database. Use this if you don't have any archive data of your own to use, though really any data from a recent paper will probably be better."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: PYDEVD_USE_CYTHON environment variable is set to 'NO'. Frame evaluator will be also disabled because it requires Cython extensions to be enabled in order to operate correctly.\n"
     ]
    }
   ],
   "source": [
    "def get_etd_data(ra, dec, url):\n",
    "    \"\"\"Imports the archive data from ETD. Uses the RA & Dec of a target to convert HJD_UTC to BJD_TDB.\n",
    "    Inputs:\n",
    "    - ra, the RA in hh mm ss.ss\n",
    "    - dec, the Declination in hh mm ss.ss\n",
    "    - url, the URL of datafile\n",
    "    Outputs:\n",
    "    - no, the number of the entry in ETD's list\n",
    "    - bjd, t_mid converted to BJD_TDB\n",
    "    - mid_err, the uncertainty in t_mid\n",
    "    - epoch, the epoch number for the transit based on the t_0 and period on ETD.\n",
    "    - dq, ETD's Data Quality measurement. 1 is best, 5 is worst.\n",
    "    \"\"\"\n",
    "    # TODO import the epoch and period guess from ETD automatically, perhaps using split?\n",
    "\n",
    "    # Import the data into a series of arrays using numpy loadtxt. Note we skip the first 5 rows and only use\n",
    "    # certain columns, because there's lots of extra stuff we aren't using\n",
    "    no,hjd,mid_err,epoch,dq = np.genfromtxt(url, encoding='unicode_escape', delimiter=';', skip_header=5, missing_values='', filling_values=-1, usecols=(0,1,2,3,10), unpack=True)\n",
    "\n",
    "    # convert from the truncated HJD to the full HJD\n",
    "    hjd = hjd + 2400000\n",
    "\n",
    "    # assuming ETD uses HJD_UTC, convert to BJD_TDB.\n",
    "    # of course, it might be worth checking that each individual data point specified is ACTUALLY HJD,\n",
    "    # as I don't think ETD vets for those kinds of mistakes\n",
    "\n",
    "    ra_dec_string = ra+\" \"+dec #concatenate into one string for the hh mm ss.ss to deg convert function\n",
    "    ra, dec = plc.ra_dec_string_to_deg(ra_dec_string) # convert from hh mm ss.ss to degrees\n",
    "\n",
    "    # Convert to BJD_TDB. N.B. we assume uncertainty remains the same.\n",
    "    bjd = np.zeros(len(hjd)) # make a zeros array to store BJD in\n",
    "        for i in tqdm(range(0, len(hjd))):\n",
    "        bjd[i] = plc.hjd_utc_to_bjd_tdb(ra, dec, hjd[i]) # do the actual converting\n",
    "\n",
    "    # return the data arrays. Notice this means you end up with five arrays, where each of the five\n",
    "    # elements is an array for number, BJD, uncertainty, epoch, DQ.\n",
    "    # Yes, this is a dreadful situation to be in. A high priority todo is to rewrite these into \"datapoint\" objects\n",
    "    # where each object stores a mid-transit time and uncertainty, its number, epoch, and DQ.\n",
    "    return no, bjd, mid_err,epoch,dq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def filter_etd_data(no, bjd, mid_err, epoch, dq, filter_dq):\n",
    "    \"\"\"Filters ETD data. Checks that uncertainty exists. Returns only datapoints with DQ equal to or better than specified value. N.B. 1 to 5 is best to worst\n",
    "    Inputs:\n",
    "    - no\n",
    "    - bjd\n",
    "    - mid_err\n",
    "    - epoch\n",
    "    - dq\n",
    "    - filter_dq\n",
    "    Outputs:\n",
    "    - filteredData, the filtered data array (of arrays). Contains three arrays: BJD mid-time, the uncertainty, and the epoch\"\"\"\n",
    "\n",
    "    # Filter for \"good\" data. I don't trust data without a tmid uncertainty, so check it exists/isn't 0\n",
    "    good_data=[[],[],[],[],[]]\n",
    "\n",
    "    for i in range(0, len(no)):\n",
    "        if mid_err[i] > 0:\n",
    "            good_data[0].append(no[i])\n",
    "            good_data[1].append(bjd[i])\n",
    "            good_data[2].append(mid_err[i])\n",
    "            good_data[3].append(epoch[i])\n",
    "            good_data[4].append(dq[i])\n",
    "\n",
    "    entries = len(good_data[1]) # no of archive entries = length\n",
    "\n",
    "    # make a blank array to put into. Note we arent using np.zeros because we dont know the final\n",
    "    # length of the array as we dont know how many data points will pass the DQ filter.\n",
    "    # TODO replace all np.zeroes usage with blank arrays\n",
    "    # to recap: one array, contains three arrays - these three arrays are for mid, mid_err, epoch\n",
    "    # we don't bother preserving no, it was just imported to be used basically as a debugging tool\n",
    "    # and dq is pointless once we've filtered. This makes the addition of non-ETD data far easier\n",
    "    filteredData = [[],[],[]]\n",
    "\n",
    "    for i in range(0, entries): # for each entry in the data\n",
    "        if good_data[4][i] <= filter_dq: # check the DQ vs the specified DQ argument. If better...\n",
    "\n",
    "            # ...then add the data to each of the three arrays in filteredData that we care about\n",
    "            filteredData[0].append(good_data[1][i])\n",
    "            filteredData[1].append(good_data[2][i])\n",
    "            filteredData[2].append(good_data[3][i])\n",
    "\n",
    "    return filteredData # return the data array containing only the transits that's been filtered by DQ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Statistics functions\n",
    "\n",
    "These functions are for the $\\chi^2$ comparison. I sincerely doubt that I haven't accidentally reinvented the wheel - functions for this probably already exist. However, it's so simple, I've not exactly wasted hours on these.\n",
    "\n",
    "Equations are from \"Measurements and Their Uncertainties: A Practical Guide to Modern Error Analysis: Hughes and Hase 2010\".\n",
    "\n",
    "$$ \\chi^2 = \\sum_i{\\frac{y_i-y(x_i)}{\\alpha_i^2}}$$\n",
    "\n",
    "$$\\chi^2_\\text{reduced} = \\frac{\\chi^2}{\\nu} $$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def chi_squared(y,yx,alpha):\n",
    "    \"\"\"Calculates the unreduced chi squared.\n",
    "    Inputs:\n",
    "    - y, the actual observed y value (the y_i in the formula)\n",
    "    - yx, the y value of the fitted line (the y(x) in the formula )\n",
    "    - alpha, the error bar for y, in the same units as y\n",
    "    Outputs:\n",
    "    - chi2, the unreduced chi squared value\n",
    "    \"\"\"\n",
    "\n",
    "    chi2=0\n",
    "    for i in range(0,len(y)):\n",
    "        chi2 += ((y[i]-yx[i])**2) / (alpha[i]**2)\n",
    "    return chi2\n",
    "\n",
    "def reduced_chi_squared(y,yx,alpha,m):\n",
    "    \"\"\"Calculates the reduced chi squared from the raw data: just chisq divided by degrees of freedom\n",
    "    Degree of freedom is just number of observations n - number of fitted parameters m\n",
    "    where n is just the number of y (or y(x) or x) values\n",
    "    Inputs:\n",
    "    - y, the actual observed y value (the y_i in the formula)\n",
    "    - yx, the y value of the fitted line (the y(x) in the formula )\n",
    "    - alpha, the error bar for y, in the same units as y\n",
    "    - m, the number of fitted parameters.\n",
    "    Outputs:\n",
    "    - chi2, the unreduced chi squared value\n",
    "    \"\"\"\n",
    "    n = len(y) # number of observations/datapoints\n",
    "    dof = n-m # degree of freedom, m is fitted params\n",
    "    chi2 = chi_squared(y,yx,alpha) # get the unreduced chi squared\n",
    "    reduced_chi2 = chi2/dof # reduce it\n",
    "    return reduced_chi2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}