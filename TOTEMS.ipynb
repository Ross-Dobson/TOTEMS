{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TOTEMS - Tidal Orbital decay Timing Extrapolation & Modelling Software"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading oec database...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Import curve fitting (Which is what we need for tidal decay)\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "#Import pylightcurve, used for BJD HJD conversions - thanks to Angelos Tsiaras\n",
    "import pylightcurve as plc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%matplotlib notebook\n",
    "# if this isn't in a separate cell, sometimes doesn't work right\n",
    "# magic commands are weird, to say the least.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "First function: gets the relevant data for the exoplanet from the [http://var2.astro.cz/ETD/](Exoplanet Transit\n",
    "Database). Use this if you don't\n",
    "have any archive data of your own to use, though really any data from a recent paper will probably be better."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_etd_data(right_ascension, declination, url):\n",
    "    \"\"\"Imports the archive data from ETD. Uses the RA & Dec of a target to convert HJD_UTC to BJD_TDB.\n",
    "    Inputs:\n",
    "    - ra, the RA in hh mm ss.ss\n",
    "    - dec, the Declination in hh mm ss.ss\n",
    "    - url, the URL of datafile\n",
    "    Outputs:\n",
    "    - no, the number of the entry in ETD's list\n",
    "    - bjd, t_mid converted to BJD_TDB\n",
    "    - mid_err, the uncertainty in t_mid\n",
    "    - epoch, the epoch number for the transit based on the t_0 and period on ETD.\n",
    "    - dq, ETD's Data Quality measurement. 1 is best, 5 is worst.\n",
    "    \"\"\"\n",
    "    # Get the period and mid-time @ epoch 0 automatically from ETD\n",
    "    p_0, t_0 = p0_t0_from_etd(url)\n",
    "\n",
    "    # Now for the actual transit data: use genfromtxt as there may be missing values.\n",
    "    # Import the data into a series of arrays using numpy loadtxt. Note we skip the first 5 rows and only use\n",
    "    # certain columns, because there's lots of extra stuff we aren't using\n",
    "    no,hjd,mid_err,epoch,dq = np.genfromtxt(url, encoding='unicode_escape', delimiter=';', skip_header=5, missing_values='', filling_values=-1, usecols=(0,1,2,3,10), unpack=True)\n",
    "\n",
    "    # convert from the truncated HJD to the full HJD\n",
    "    hjd = hjd + 2400000\n",
    "\n",
    "    # assuming ETD uses HJD_UTC, convert to BJD_TDB.\n",
    "    # of course, it might be worth checking that each individual data point specified is ACTUALLY HJD,\n",
    "    # as I don't think ETD vets for those kinds of mistakes\n",
    "\n",
    "    ra_dec_string = right_ascension+\" \"+declination #concatenate into one string, for fn converting to degrees\n",
    "    right_ascension, declination = plc.ra_dec_string_to_deg(ra_dec_string) # convert from hh mm ss.ss to degrees\n",
    "\n",
    "    # Convert to BJD_TDB. N.B. we assume uncertainty remains the same.\n",
    "    bjd = [] # make a blank array to store BJD in\n",
    "    for hjd_to_convert in hjd:\n",
    "        bjd.append(plc.hjd_utc_to_bjd_tdb(ra, dec, hjd_to_convert)) # do the actual converting\n",
    "\n",
    "    # return the data arrays. Notice this means you end up with five arrays, where each of the five elements is an\n",
    "    # array for number, BJD, uncertainty, epoch, DQ. Also, two constants, the period and time at epoch 0.\n",
    "    # Yes, this is a dreadful situation to be in. A high priority todo is to rewrite these into \"datapoint\" objects\n",
    "    # where each object stores a mid-transit time and uncertainty, its number, epoch, and DQ.\n",
    "    return no, bjd, mid_err,epoch,dq, p_0, t_0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def p0_t0_from_etd(url):\n",
    "    \"\"\"Uses ETD to get values of t_0 and p_0, the mid-time and period at epoch 0.\n",
    "    Inputs:\n",
    "    -url, the url of the ETD data in ASCII format\n",
    "    Outputs:\n",
    "    -t_0, the mid-time @ epoch=0\n",
    "    -p_0, the period @ epoch=0\"\"\"\n",
    "    p_str, t_str = np.loadtxt(url, dtype=str, encoding='unicode_escape', delimiter=', ', skiprows=2, max_rows=1)\n",
    "\n",
    "    for possibility in p_str.split():\n",
    "        try:\n",
    "            str(float(possibility))\n",
    "            p_0 = possibility\n",
    "        except ValueError:\n",
    "            pass # if its not the number, move on\n",
    "\n",
    "    for possibility in t_str.split():\n",
    "        try:\n",
    "            str(float(possibility))\n",
    "            t_0 = possibility\n",
    "        except ValueError:\n",
    "            pass # if its not the number, move on\n",
    "\n",
    "    return p_0, t_0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def filter_etd_data(no, bjd, mid_err, epoch, dq, filter_dq):\n",
    "    \"\"\"Filters ETD data. Checks uncertainty exists. Returns only datapoints with DQ equal or better than specified\n",
    "    value. N.B. DQ 1 is best, 5 is worst.\n",
    "    Inputs:\n",
    "    - no\n",
    "    - bjd\n",
    "    - mid_err\n",
    "    - epoch\n",
    "    - dq\n",
    "    - filter_dq\n",
    "    Outputs:\n",
    "    - filteredData, the filtered data array (of arrays). Contains three arrays: BJD mid-time, uncertainty, and epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Let's filter for \"good\" data. I don't trust data without a tmid uncertainty, so check it exists/isn't 0\n",
    "    good_data=[ [],[],[],[],[] ]\n",
    "    for i,err in enumerate(mid_err):\n",
    "        if err < 0:\n",
    "            good_data[0].append(no[i])\n",
    "            good_data[1].append(bjd[i])\n",
    "            good_data[2].append(mid_err[i])\n",
    "            good_data[3].append(epoch[i])\n",
    "            good_data[4].append(dq[i])\n",
    "\n",
    "    # the plan: one array, contains three arrays - these three arrays are for mid, mid_err, epoch\n",
    "    # we don't bother preserving no., it was just imported to be used basically as a debugging tool\n",
    "    # and dq is pointless once we've filtered. This also makes the addition of non-ETD data far easier\n",
    "    filtered_data = [[],[],[]]\n",
    "\n",
    "    for i,good_dq in enumerate(good_data[4]): # for each entry in the data\n",
    "        if good_dq <= filter_dq: # check the DQ vs the specified DQ argument. If better...\n",
    "\n",
    "            # ...then add the data to each of the three arrays in filteredData that we care about\n",
    "            filtered_data[0].append(good_data[1][i])\n",
    "            filtered_data[1].append(good_data[2][i])\n",
    "            filtered_data[2].append(good_data[3][i])\n",
    "\n",
    "    return filtered_data # return the data array containing only the transits that's been filtered by DQ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Statistics functions\n",
    "\n",
    "These functions are for the $\\chi^2$ comparison. I sincerely doubt that I haven't accidentally reinvented the wheel - functions for this probably already exist. However, it's so simple, I've not exactly wasted hours on these.\n",
    "\n",
    "Equations are from \"Measurements and Their Uncertainties: A Practical Guide to Modern Error Analysis: Hughes and\n",
    "Hase 2010\".\n",
    "\n",
    "$$ \\chi^2 = \\sum_i{\\frac{y_i-y(x_i)}{\\alpha_i^2}}$$\n",
    "\n",
    "$\\nu$ is the degrees of freedom: the number of datapoints minus the number of fitted parameters. We divide $\\chi^2$ by\n",
    " $\\nu$ to obtain the reduced chi-squared, as follows:\n",
    "\n",
    "$$ \\chi^2_\\text{reduced} = \\frac{\\chi^2}{\\nu} $$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def chi_squared(y_i,yx,alpha):\n",
    "    \"\"\"Calculates the unreduced chi squared.\n",
    "    Inputs:\n",
    "    - y_i, the array of observed y value (the y_i in the formula)\n",
    "    - yx, the array of y values of the fitted line (the y(x) in the formula )\n",
    "    - alpha, array of error in y, in the same units as y\n",
    "    Outputs:\n",
    "    - chi2, the unreduced chi squared value\n",
    "    \"\"\"\n",
    "\n",
    "    chi2=0\n",
    "    for i,y in enumerate(y_i):\n",
    "        chi2 += ((y-yx[i])**2) / (alpha[i]**2)\n",
    "    return chi2\n",
    "\n",
    "def reduced_chi_squared(y,yx,alpha,m):\n",
    "    \"\"\"Calculates the reduced chi squared from the raw data: just chisq divided by degrees of freedom\n",
    "    Degree of freedom is just number of observations n - number of fitted parameters m\n",
    "    where n is just the number of y (or y(x) or x) values\n",
    "    Inputs:\n",
    "    - y, array of actual observed y value (the y_i in the formula)\n",
    "    - yx, array of y value of the fitted line (the y(x) in the formula )\n",
    "    - alpha, array of error in y, in the same units as y\n",
    "    - m, the number of fitted parameters.\n",
    "    Outputs:\n",
    "    - chi2, the unreduced chi squared value\n",
    "    \"\"\"\n",
    "    n = len(y) # number of observations/datapoints\n",
    "    dof = n-m # degree of freedom, m is fitted params\n",
    "    chi2 = chi_squared(y,yx,alpha) # get the unreduced chi squared\n",
    "    reduced_chi2 = chi2/dof # reduce it\n",
    "    return reduced_chi2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# TODO make transit objects, same with ETD data. OO principles need to be adopted rather than this mess\n",
    "# TODO error handling and exceptions need to be added, because this too is currently a mess.\n",
    "\n",
    "def  add_own_data(filename):\n",
    "    \"\"\"Function to add archive data and your own data. Put it into three columns: tmid, error, and type of JD (Either\n",
    "     \"HJD\" or \"BJD\", assuming HJD_UTC or BJD_TDB. Will be asked for the P_0: supply your own or use ETDs. For t_0,\n",
    "     assumes earliest tmid given. NOTE: if ETD data has been used, it will force the ETD values.\n",
    "    Inputs:\n",
    "    - filename, the name/path of your data table (just use a csv)\n",
    "    Outputs:\n",
    "    - bjd, array of BJD_TDB mid-transit-times\n",
    "    - mid_err, the uncertainty of those tmids\n",
    "    - epoch, transits since t_0\n",
    "    - t_0, midtime at epoch=0\n",
    "    - p_0, the period at epoch=0\n",
    "    \"\"\"\n",
    "    # TODO don't do this\n",
    "    global url\n",
    "    global ra\n",
    "    global dec\n",
    "\n",
    "    # import all the data\n",
    "    mid,mid_err,jd_type = np.genfromtxt(filename, encoding='unicode_escape', delimiter=',', missing_values='',\n",
    "                                            filling_values=-1, unpack=True)\n",
    "\n",
    "    bjd=[]\n",
    "    # check if we need to convert to BJD\n",
    "    # TODO: error handling needs implementing here\n",
    "    for i,tmid in enumerate(mid):\n",
    "        if jd_type[i] == \"BJD\":\n",
    "            bjd.append(tmid)\n",
    "        elif jd_type[i] == \"HJD\":\n",
    "            bjd.append(plc.hjd_utc_to_bjd_tdb(ra, dec, tmid))\n",
    "        else:\n",
    "            print(\"Something has gone wrong! Your type value needs to be a string of BJD or HJD.\")\n",
    "            # TODO: better way to deal with this, proper exception handling\n",
    "            break\n",
    "\n",
    "    # check if we need to assume a t_0 and p_0, if so, do so\n",
    "    t_0 = 0\n",
    "    p_0 = 0\n",
    "    if not etd_used:\n",
    "        # use the earliest tmid as t_0\n",
    "        t_0 = np.amin(bjd)\n",
    "\n",
    "        # ask for p_0, check it's sensible\n",
    "        p_good = False\n",
    "        while not p_good:\n",
    "            p_0 = input(\"Please input the value of P, in days:\")\n",
    "            if p_0 > 0:\n",
    "                p_good = True\n",
    "                # TODO: a better check than this!\n",
    "    else:\n",
    "        # url should be defined already outside this function, so can just call it because we're bad people that\n",
    "        # aren't obeying proper OO principles\n",
    "        # TODO: obey proper OO principles\n",
    "        p_0, t_0 = p0_t0_from_etd(url)\n",
    "\n",
    "    epoch=[]\n",
    "\n",
    "    # calculate the epoch system - assume closest rounding is correct via np.rint\n",
    "    for i,tmid in enumerate(bjd):\n",
    "        epoch[i] = np.rint((tmid-t_0)/p_0)\n",
    "\n",
    "    #return everything: again, three arrays, two floats\n",
    "    return bjd, mid_err, epoch, p_0, t_0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Working example: WASP-12b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/TOTEMS/lib/python3.8/site-packages/scipy/optimize/minpack.py:828: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  warnings.warn('Covariance of the parameters could not be estimated',\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "example_wasp12_data.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-89ef334f780b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;31m# Add own archive data, from an example file.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 20\u001B[0;31m \u001B[0mwasp12_own_bjd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwasp12_own_mid_err\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwasp12_own_epoch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwasp12_p_0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwasp12_t_0\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0madd_own_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'example_wasp12_data.csv'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-7-61cc7bc6fe8e>\u001B[0m in \u001B[0;36madd_own_data\u001B[0;34m(filename)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m     \u001B[0;31m# import all the data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 19\u001B[0;31m     mid,mid_err,jd_type = np.genfromtxt(filename, encoding='unicode_escape', delimiter=',', missing_values='',\n\u001B[0m\u001B[1;32m     20\u001B[0m                                             filling_values=-1, unpack=True)\n\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/TOTEMS/lib/python3.8/site-packages/numpy/lib/npyio.py\u001B[0m in \u001B[0;36mgenfromtxt\u001B[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding)\u001B[0m\n\u001B[1;32m   1770\u001B[0m             \u001B[0mfname\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos_fspath\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1771\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbasestring\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1772\u001B[0;31m             \u001B[0mfid\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_datasource\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'rt'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mencoding\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1773\u001B[0m             \u001B[0mfid_ctx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcontextlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclosing\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfid\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1774\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/TOTEMS/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001B[0m in \u001B[0;36mopen\u001B[0;34m(path, mode, destpath, encoding, newline)\u001B[0m\n\u001B[1;32m    267\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    268\u001B[0m     \u001B[0mds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDataSource\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdestpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 269\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mencoding\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnewline\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnewline\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    270\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    271\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/envs/TOTEMS/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001B[0m in \u001B[0;36mopen\u001B[0;34m(self, path, mode, encoding, newline)\u001B[0m\n\u001B[1;32m    621\u001B[0m                                       encoding=encoding, newline=newline)\n\u001B[1;32m    622\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 623\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mIOError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"%s not found.\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    624\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOSError\u001B[0m: example_wasp12_data.csv not found."
     ]
    }
   ],
   "source": [
    "etd_used = False\n",
    "\n",
    "wasp12_ra = \"06 30 32.79\"\n",
    "wasp12_dec = \"+29 40 20.20\"\n",
    "wasp12_etd_url = \"http://var2.astro.cz/ETD/ascii-etd.php?id=246&STARNAME=WASP-12&PLANET=b&PER=1.0914222&EPOCH=2454508\" \\\n",
    "              \".97605\"\n",
    "\n",
    "# Get data from ETD\n",
    "wasp12_etd_no, wasp12_etd_bjd, wasp12_etd_mid_err, wasp12_etd_epoch, wasp12_etd_dq, wasp12_p_0, wasp12_t_0 = \\\n",
    "    get_etd_data(wasp12_ra, wasp12_dec, wasp12_etd_url)\n",
    "etd_used = True\n",
    "\n",
    "# Filter the data - let's use DQ 4, just to exclude only the worst data\n",
    "DQ_filter = 4\n",
    "wasp12_etd_data = filter_etd_data(wasp12_etd_no, wasp12_etd_bjd, wasp12_etd_mid_err, wasp12_etd_epoch, wasp12_etd_dq,\n",
    "                               DQ_filter)\n",
    "\n",
    "# So, now we have the period and mid-time @ epoch 0, and we have the mid-times, uncertainties, and epochs.\n",
    "\n",
    "# Add own archive data, from an example file.\n",
    "path_to_own_data=\"example_data.csv\"\n",
    "wasp12_own_bjd, wasp12_own_mid_err, wasp12_own_epoch, wasp12_p_0, wasp12_t_0 = add_own_data(path_to_own_data)\n",
    "wasp12_own_data = [wasp12_own_bjd, wasp12_own_mid_err, wasp12_own_epoch]\n",
    "\n",
    "# Now we need to construct the final array of data, sorted by epoch, for plotting:\n",
    "# (Obviously, if just using ETD or just using archive, comment this whole section out\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sort_etd_own_data(etd_data, own_data):\n",
    "    \"\"\"Sorts the filtered ETD data and user's archive data into the correct order.\n",
    "    Inputs:\n",
    "    -etd_data, an array of ETD data\n",
    "    -own_data, archive data added from literature, may contain user's own datapoint\n",
    "    Outputs:\n",
    "    -sorted_data, the sorted combination of these two datasets\n",
    "    \"\"\"\n",
    "    own_bjd, own_mid_err, own_epoch = own_data\n",
    "    etd_bjd, etd_mid_err, etd_epoch = etd_data\n",
    "    for j in tqdm(range(0, len(own_epoch))):\n",
    "        for i in range(0, len(etd_epoch)):\n",
    "            if (own_epoch[j] >= etd_epoch[i]):\n",
    "                etd_epoch = np.insert(etd_epoch,i,own_epoch[j])\n",
    "                etd_bjd = np.insert(etd_bjd,i,own_bjd[j])\n",
    "                etd_mid_err = np.insert(etd_mid_err,i,own_mid_err[j])\n",
    "                break\n",
    "    # TODO: inefficient, better to add to new array than constantly cycling ETD array?\n",
    "\n",
    "    return [etd_epoch, etd_bjd, etd_mid_err]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "1\n",
      "20\n",
      "2\n",
      "30\n",
      "3\n",
      "40\n",
      "4\n",
      "50\n",
      "5\n",
      "60\n",
      "6\n",
      "40\n",
      "7\n",
      "30\n",
      "8\n",
      "20\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}