{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TOTEMS - Tidal Orbital decay Timing Extrapolation & Modelling Software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Import curve fitting (Which is what we need for tidal decay)\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "#Import pylightcurve, used for BJD HJD conversions - thanks to Angelos Tsiaras\n",
    "import pylightcurve as plc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "# if this isn't in a separate cell, sometimes doesn't work right\n",
    "# magic commands are weird, to say the least.\n",
    "\n",
    "# TODO - use structured or recorded arrays with boolean masks, rather than repeatedly iterating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First function: gets the relevant data for the exoplanet from the [http://var2.astro.cz/ETD/](Exoplanet Transit\n",
    "Database). Use this if you don't\n",
    "have any archive data of your own to use, though really any data from a recent paper will probably be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_etd_data(right_ascension, declination, url):\n",
    "    \"\"\"Imports the archive data from ETD. Uses the RA & Dec of a target to convert HJD_UTC to BJD_TDB.\n",
    "    Inputs:\n",
    "    - ra, the RA in hh mm ss.ss\n",
    "    - dec, the Declination in hh mm ss.ss\n",
    "    - url, the URL of datafile\n",
    "    Outputs:\n",
    "    - no, the number of the entry in ETD's list\n",
    "    - bjd, t_mid converted to BJD_TDB\n",
    "    - mid_err, the uncertainty in t_mid\n",
    "    - epoch, the epoch number for the transit based on the t_0 and period on ETD.\n",
    "    - dq, ETD's Data Quality measurement. 1 is best, 5 is worst.\n",
    "    \"\"\"\n",
    "    # Get the period and mid-time @ epoch 0 automatically from ETD\n",
    "    p_0, t_0 = p0_t0_from_etd(url)\n",
    "\n",
    "    # Now for the actual transit data: use genfromtxt as there may be missing values.\n",
    "    # Import the data into a series of arrays using numpy loadtxt. Note we skip the first 5 rows and only use\n",
    "    # certain columns, because there's lots of extra stuff we aren't using\n",
    "    no,hjd,mid_err,epoch,dq = np.genfromtxt(url, encoding='unicode_escape', delimiter=';', skip_header=5, missing_values='', filling_values=-1, usecols=(0,1,2,3,10), unpack=True)\n",
    "\n",
    "    # convert from the truncated HJD to the full HJD\n",
    "    hjd = hjd + 2400000\n",
    "\n",
    "    # assuming ETD uses HJD_UTC, convert to BJD_TDB.\n",
    "    # of course, it might be worth checking that each individual data point specified is ACTUALLY HJD,\n",
    "    # as I don't think ETD vets for those kinds of mistakes\n",
    "\n",
    "    ra_dec_string = right_ascension+\" \"+declination #concatenate into one string, for fn converting to degrees\n",
    "    right_ascension, declination = plc.ra_dec_string_to_deg(ra_dec_string) # convert from hh mm ss.ss to degrees\n",
    "\n",
    "    # Convert to BJD_TDB. N.B. we assume uncertainty remains the same.\n",
    "    bjd = [] # make a blank array to store BJD in\n",
    "    for hjd_to_convert in hjd:\n",
    "        bjd.append(plc.hjd_utc_to_bjd_tdb(ra, dec, hjd_to_convert)) # do the actual converting\n",
    "\n",
    "    # return the data arrays. Notice this means you end up with five arrays, where each of the five elements is an\n",
    "    # array for number, BJD, uncertainty, epoch, DQ. Also, two constants, the period and time at epoch 0.\n",
    "    # Yes, this is a dreadful situation to be in. A high priority todo is to rewrite these into \"datapoint\" objects\n",
    "    # where each object stores a mid-transit time and uncertainty, its number, epoch, and DQ.\n",
    "    return no, bjd, mid_err,epoch,dq, p_0, t_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def p0_t0_from_etd(url):\n",
    "    \"\"\"Uses ETD to get values of t_0 and p_0, the mid-time and period at epoch 0.\n",
    "    Inputs:\n",
    "    -url, the url of the ETD data in ASCII format\n",
    "    Outputs:\n",
    "    -t_0, the mid-time @ epoch=0\n",
    "    -p_0, the period @ epoch=0\"\"\"\n",
    "    p_str, t_str = np.loadtxt(url, dtype=str, encoding='unicode_escape', delimiter=', ', skiprows=2, max_rows=1)\n",
    "\n",
    "    for possibility in p_str.split():\n",
    "        try:\n",
    "            str(float(possibility))\n",
    "            p_0 = possibility\n",
    "        except ValueError:\n",
    "            pass # if its not the number, move on\n",
    "\n",
    "    for possibility in t_str.split():\n",
    "        try:\n",
    "            str(float(possibility))\n",
    "            t_0 = possibility\n",
    "        except ValueError:\n",
    "            pass # if its not the number, move on\n",
    "\n",
    "    return p_0, t_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_etd_data(no, bjd, mid_err, epoch, dq, filter_dq):\n",
    "    \"\"\"Filters ETD data. Checks uncertainty exists. Returns only datapoints with DQ equal or better than specified\n",
    "    value. N.B. DQ 1 is best, 5 is worst.\n",
    "    Inputs:\n",
    "    - no\n",
    "    - bjd\n",
    "    - mid_err\n",
    "    - epoch\n",
    "    - dq\n",
    "    - filter_dq\n",
    "    Outputs:\n",
    "    - filteredData, the filtered data array (of arrays). Contains three arrays: BJD mid-time, uncertainty, and epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Let's filter for \"good\" data. I don't trust data without a tmid uncertainty, so check it exists/isn't 0\n",
    "    good_data=[ [],[],[],[],[] ]\n",
    "    for i,err in enumerate(mid_err):\n",
    "        if err < 0:\n",
    "            good_data[0].append(no[i])\n",
    "            good_data[1].append(bjd[i])\n",
    "            good_data[2].append(mid_err[i])\n",
    "            good_data[3].append(epoch[i])\n",
    "            good_data[4].append(dq[i])\n",
    "\n",
    "    # the plan: one array, contains three arrays - these three arrays are for mid, mid_err, epoch\n",
    "    # we don't bother preserving no., it was just imported to be used basically as a debugging tool\n",
    "    # and dq is pointless once we've filtered. This also makes the addition of non-ETD data far easier\n",
    "    filtered_data = [[],[],[]]\n",
    "\n",
    "    for i,good_dq in enumerate(good_data[4]): # for each entry in the data\n",
    "        if good_dq <= filter_dq: # check the DQ vs the specified DQ argument. If better...\n",
    "\n",
    "            # ...then add the data to each of the three arrays in filteredData that we care about\n",
    "            filtered_data[0].append(good_data[1][i])\n",
    "            filtered_data[1].append(good_data[2][i])\n",
    "            filtered_data[2].append(good_data[3][i])\n",
    "\n",
    "    return filtered_data # return the data array containing only the transits that's been filtered by DQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Statistics functions\n",
    "\n",
    "These functions are for the $\\chi^2$ comparison. I sincerely doubt that I haven't accidentally reinvented the wheel - functions for this probably already exist. However, it's so simple, I've not exactly wasted hours on these.\n",
    "\n",
    "Equations are from \"Measurements and Their Uncertainties: A Practical Guide to Modern Error Analysis: Hughes and\n",
    "Hase 2010\".\n",
    "\n",
    "$$ \\chi^2 = \\sum_i{\\frac{y_i-y(x_i)}{\\alpha_i^2}}$$\n",
    "\n",
    "$\\nu$ is the degrees of freedom: the number of datapoints minus the number of fitted parameters. We divide $\\chi^2$ by\n",
    " $\\nu$ to obtain the reduced chi-squared, as follows:\n",
    "\n",
    "$$ \\chi^2_\\text{reduced} = \\frac{\\chi^2}{\\nu} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def chi_squared(y_i,yx,alpha):\n",
    "    \"\"\"Calculates the unreduced chi squared.\n",
    "    Inputs:\n",
    "    - y_i, the array of observed y value (the y_i in the formula)\n",
    "    - yx, the array of y values of the fitted line (the y(x) in the formula )\n",
    "    - alpha, array of error in y, in the same units as y\n",
    "    Outputs:\n",
    "    - chi2, the unreduced chi squared value\n",
    "    \"\"\"\n",
    "\n",
    "    chi2=0\n",
    "    for i,y in enumerate(y_i):\n",
    "        chi2 += ((y-yx[i])**2) / (alpha[i]**2)\n",
    "    return chi2\n",
    "\n",
    "def reduced_chi_squared(y,yx,alpha,m):\n",
    "    \"\"\"Calculates the reduced chi squared from the raw data: just chisq divided by degrees of freedom\n",
    "    Degree of freedom is just number of observations n - number of fitted parameters m\n",
    "    where n is just the number of y (or y(x) or x) values\n",
    "    Inputs:\n",
    "    - y, array of actual observed y value (the y_i in the formula)\n",
    "    - yx, array of y value of the fitted line (the y(x) in the formula )\n",
    "    - alpha, array of error in y, in the same units as y\n",
    "    - m, the number of fitted parameters.\n",
    "    Outputs:\n",
    "    - chi2, the unreduced chi squared value\n",
    "    \"\"\"\n",
    "    n = len(y) # number of observations/datapoints\n",
    "    dof = n-m # degree of freedom, m is fitted params\n",
    "    chi2 = chi_squared(y,yx,alpha) # get the unreduced chi squared\n",
    "    reduced_chi2 = chi2/dof # reduce it\n",
    "    return reduced_chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO stop using globals. Consider OO principles, or structured/recorded arrays.\n",
    "# TODO error handling and exceptions need to be added, because this too is currently a mess.\n",
    "\n",
    "def  add_own_data(filename):\n",
    "    \"\"\"Function to add archive data and your own data. Put it into three columns: tmid, error, and type of JD (Either\n",
    "     \"HJD\" or \"BJD\", assuming HJD_UTC or BJD_TDB. Will be asked for the P_0: supply your own or use ETDs. For t_0,\n",
    "     assumes earliest tmid given. NOTE: if ETD data has been used, it will force the ETD values.\n",
    "    Inputs:\n",
    "    - filename, the name/path of your data table (just use a csv)\n",
    "    Outputs:\n",
    "    - bjd, array of BJD_TDB mid-transit-times\n",
    "    - mid_err, the uncertainty of those tmids\n",
    "    - epoch, transits since t_0\n",
    "    - t_0, midtime at epoch=0\n",
    "    - p_0, the period at epoch=0\n",
    "    \"\"\"\n",
    "    # TODO don't do this\n",
    "    global url\n",
    "    global ra\n",
    "    global dec\n",
    "\n",
    "    # import all the data\n",
    "    mid,mid_err,jd_type = np.genfromtxt(filename, encoding='unicode_escape', delimiter=',', missing_values='',\n",
    "                                            filling_values=-1, unpack=True)\n",
    "\n",
    "    bjd=[]\n",
    "    # check if we need to convert to BJD\n",
    "    # TODO: error handling needs implementing here\n",
    "    for i,tmid in enumerate(mid):\n",
    "        if jd_type[i] == \"BJD\":\n",
    "            bjd.append(tmid)\n",
    "        elif jd_type[i] == \"HJD\":\n",
    "            bjd.append(plc.hjd_utc_to_bjd_tdb(ra, dec, tmid))\n",
    "        else:\n",
    "            print(\"Something has gone wrong! Your type value needs to be a string of BJD or HJD.\")\n",
    "            # TODO: better way to deal with this, proper exception handling\n",
    "            break\n",
    "\n",
    "    # check if we need to assume a t_0 and p_0, if so, do so\n",
    "    t_0 = 0\n",
    "    p_0 = 0\n",
    "    if not etd_used:\n",
    "        # use the earliest tmid as t_0\n",
    "        t_0 = np.amin(bjd)\n",
    "\n",
    "        # ask for p_0, check it's sensible\n",
    "        p_good = False\n",
    "        while not p_good:\n",
    "            p_0 = input(\"Please input the value of P, in days:\")\n",
    "            if p_0 > 0:\n",
    "                p_good = True\n",
    "                # TODO: a better check than this!\n",
    "    else:\n",
    "        # url should be defined already outside this function, so can just call it because we're bad people that\n",
    "        # aren't obeying proper OO principles\n",
    "        # TODO: obey proper OO principles\n",
    "        p_0, t_0 = p0_t0_from_etd(url)\n",
    "\n",
    "    epoch=[]\n",
    "\n",
    "    # calculate the epoch system - assume closest rounding is correct via np.rint\n",
    "    for i,tmid in enumerate(bjd):\n",
    "        epoch[i] = np.rint((tmid-t_0)/p_0)\n",
    "\n",
    "    #return everything: again, three arrays, two floats\n",
    "    return bjd, mid_err, epoch, p_0, t_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sort_etd_own_data(etd_data, own_data):\n",
    "    \"\"\"Sorts the filtered ETD data and user's archive data into the correct order.\n",
    "    Inputs:\n",
    "    -etd_data, an array of ETD data\n",
    "    -own_data, archive data added from literature, may contain user's own datapoint\n",
    "    Outputs:\n",
    "    -sorted_data, the sorted combination of these two datasets\n",
    "    \"\"\"\n",
    "    own_bjd, own_mid_err, own_epoch = own_data\n",
    "    etd_bjd, etd_mid_err, etd_epoch = etd_data\n",
    "    for j in tqdm(range(0, len(own_epoch))):\n",
    "        for i in range(0, len(etd_epoch)):\n",
    "            if (own_epoch[j] >= etd_epoch[i]):\n",
    "                etd_epoch = np.insert(etd_epoch,i,own_epoch[j])\n",
    "                etd_bjd = np.insert(etd_bjd,i,own_bjd[j])\n",
    "                etd_mid_err = np.insert(etd_mid_err,i,own_mid_err[j])\n",
    "                break\n",
    "    # TODO: inefficient, better to add to new array than constantly cycling ETD array?\n",
    "\n",
    "    return [etd_epoch, etd_bjd, etd_mid_err]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/opt/anaconda3/envs/TOTEMS/bin/python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "name": "TOTEMS.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
